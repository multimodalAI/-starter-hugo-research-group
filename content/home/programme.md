---
# A Demo section created with the Blank widget.
# Any elements can be added in the body: https://wowchemy.com/docs/writing-markdown-latex/
# Add more sections by duplicating this file and customizing to your requirements.
widget: blank
headless: true # This file represents a page section.

weight: 30 # Order that this section will appear.
title:
hero_media: 
design:
  # Choose how many columns the section has. Valid values: 1 or 2.
  columns: '1'
  # Add custom styles
  css_style:
  css_class:
---

This online forum will feature:

- A keynote presentation "**[LLaVA: A Vision-Language Approach to Computer Vision in the Wild](https://llava-vl.github.io/)**" by Chunyuan Li, Research Lead at ByteDance/TikTok, (Co-)Lead Developer of [LLaVA](https://llava.hliu.cc/), Former Principal Researcher at Microsoft Research, Redmond.

  **Abstract :** The future of AI is in creating systems like foundation models that are pre-trained once, and will handle countless many downstream tasks directly (zero-shot), or adapt to new tasks quickly (few-shot). In this talk, I will discuss our vision-language approach to achieving “Computer Vision in the Wild (CVinW)”:  building such a transferable system in computer vision (CV) that can effortlessly generalize to a wide range of visual recognition tasks in the wild. I will dive into Large Language-and-Vision Assistant (LLaVA) and its series, which is the first open-source project to exhibit the GPT-V level capabilities in image understanding and reasoning. I will demonstrate a promising path to build customizable large multimodal models that follow humans' intent with an affordable cost.

- An introduction to the six areas in our Multimodal AI perspective paper
- Pitches from the community
- Open discussions and conclusions
